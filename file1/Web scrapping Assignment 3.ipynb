{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4621bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.12.0-py3-none-any.whl (9.4 MB)\n",
      "     ---------------------------------------- 9.4/9.4 MB 3.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
      "     -------------------------------------- 400.2/400.2 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.10.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: outcome, h11, exceptiongroup, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed exceptiongroup-1.1.3 h11-0.14.0 outcome-1.2.0 selenium-4.12.0 trio-0.22.2 trio-websocket-0.10.3 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dccdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Ask the user for the product they want to search for\n",
    "product_name = input(\"Enter the product you want to search for on Amazon.in: \")\n",
    "\n",
    "# Create a Chrome WebDriver instance\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to Amazon.in\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "# Find the search input field and enter the product name\n",
    "search_box = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "search_box.send_keys(product_name)\n",
    "search_box.send_keys(Keys.RETURN)  # Press Enter\n",
    "\n",
    "# Wait for the search results to load (you can adjust the time as needed)\n",
    "driver.implicitly_wait(10)  # Wait for 10 seconds (adjust as needed)\n",
    "\n",
    "# Get the search results\n",
    "search_results = driver.find_elements_by_xpath('//div[@data-component-type=\"s-search-result\"]')\n",
    "\n",
    "# Loop through the search results and print product details\n",
    "for result in search_results:\n",
    "    product_title = result.find_element_by_xpath('.//span[@class=\"a-size-medium a-color-base a-text-normal\"]').text\n",
    "    product_price = result.find_element_by_xpath('.//span[@class=\"a-price-whole\"]').text\n",
    "    product_currency = result.find_element_by_xpath('.//span[@class=\"a-price-symbol\"]').text\n",
    "    product_url = result.find_element_by_xpath('.//a[@class=\"a-link-normal\"]').get_attribute(\"href\")\n",
    "\n",
    "    print(f\"Product: {product_title}\")\n",
    "    print(f\"Price: {product_currency}{product_price}\")\n",
    "    print(f\"URL: {product_url}\")\n",
    "    print()\n",
    "\n",
    "# Close the web browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape product details from a search result page\n",
    "def scrape_product_details(driver, keyword):\n",
    "    product_data = []\n",
    "    try:\n",
    "        # Wait for the page to load\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Parse the page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find all the product containers on the page\n",
    "        product_containers = soup.find_all('div', class_='s-result-item')\n",
    "\n",
    "        for container in product_containers:\n",
    "            try:\n",
    "                # Extract product details\n",
    "                brand_name = container.find('span', class_='a-size-base-plus a-color-base').text.strip()\n",
    "                product_name = container.find('span', class_='a-text-normal').text.strip()\n",
    "                price = container.find('span', class_='a-price-whole').text.strip()\n",
    "                return_exchange = container.find('span', class_='a-declarative').text.strip()\n",
    "                expected_delivery = container.find('span', class_='a-text-bold').text.strip()\n",
    "                availability = container.find('span', class_='a-size-base a-color-success').text.strip()\n",
    "                product_url = container.find('a', class_='a-link-normal')['href']\n",
    "\n",
    "                product_data.append({\n",
    "                    'Brand Name': brand_name,\n",
    "                    'Name of the Product': product_name,\n",
    "                    'Price': price,\n",
    "                    'Return/Exchange': return_exchange,\n",
    "                    'Expected Delivery': expected_delivery,\n",
    "                    'Availability': availability,\n",
    "                    'Product URL': f'https://www.amazon.in{product_url}',\n",
    "                })\n",
    "            except AttributeError:\n",
    "                # Handle missing data by adding placeholders\n",
    "                product_data.append({\n",
    "                    'Brand Name': '-',\n",
    "                    'Name of the Product': '-',\n",
    "                    'Price': '-',\n",
    "                    'Return/Exchange': '-',\n",
    "                    'Expected Delivery': '-',\n",
    "                    'Availability': '-',\n",
    "                    'Product URL': '-',\n",
    "                })\n",
    "\n",
    "        return product_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping product details: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to search for a product on Amazon\n",
    "def search_amazon_product(product_name):\n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "        # Find the search input field and enter the product name\n",
    "        search_box = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "        search_box.send_keys(product_name)\n",
    "        search_box.submit()\n",
    "\n",
    "        product_data = []\n",
    "\n",
    "        # Scrape product details from the first 3 pages\n",
    "        for page in range(3):\n",
    "            product_data += scrape_product_details(driver, product_name)\n",
    "\n",
    "            # Navigate to the next page\n",
    "            try:\n",
    "                next_button = driver.find_element_by_xpath('//li[@class=\"a-last\"]/a')\n",
    "                next_button.click()\n",
    "            except NoSuchElementException:\n",
    "                break\n",
    "\n",
    "        return product_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching for the product: {e}\")\n",
    "        return []\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    product_name = input(\"Enter the product you want to search for on Amazon.in: \")\n",
    "    product_data = search_amazon_product(product_name)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame(product_data)\n",
    "\n",
    "    # Save the data to a CSV file\n",
    "    df.to_csv(f\"{product_name}_amazon_products.csv\", index=False)\n",
    "\n",
    "    print(f\"Scraped {len(df)} products and saved to {product_name}_amazon_products.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb5b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Create a directory to save the downloaded images\n",
    "if not os.path.exists('downloaded_images'):\n",
    "    os.makedirs('downloaded_images')\n",
    "\n",
    "# Function to scrape and download images for a given keyword\n",
    "def scrape_images(keyword, num_images):\n",
    "    # Create a Chrome WebDriver instance\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        # Navigate to Google Images\n",
    "        driver.get(\"https://www.google.com/imghp\")\n",
    "\n",
    "        # Find the search input field\n",
    "        search_box = driver.find_element_by_name(\"q\")\n",
    "\n",
    "        # Enter the keyword and press Enter\n",
    "        search_box.send_keys(keyword)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Wait for the page to load\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Scroll down to load more images\n",
    "        for _ in range(3):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "        # Get the page source after scrolling\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse the page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "        # Find and download the first num_images images\n",
    "        img_tags = soup.find_all(\"img\", class_=\"rg_i\")\n",
    "        count = 0\n",
    "\n",
    "        for img_tag in img_tags:\n",
    "            img_url = img_tag.get(\"src\")\n",
    "            if img_url:\n",
    "                try:\n",
    "                    response = requests.get(img_url, stream=True)\n",
    "                    filename = f\"downloaded_images/{keyword}_{count + 1}.jpg\"\n",
    "                    with open(filename, \"wb\") as file:\n",
    "                        for chunk in response.iter_content(1024):\n",
    "                            file.write(chunk)\n",
    "                    count += 1\n",
    "                    if count >= num_images:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading image: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping images for '{keyword}': {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the web browser\n",
    "        driver.quit()\n",
    "\n",
    "# Keywords and the number of images to scrape\n",
    "keywords = [\"fruits\", \"cars\", \"Machine Learning\", \"Guitar\", \"Cakes\"]\n",
    "num_images = 10\n",
    "\n",
    "# Scrape images for each keyword\n",
    "for keyword in keywords:\n",
    "    print(f\"Scraping images for keyword: {keyword}\")\n",
    "    scrape_images(keyword, num_images)\n",
    "\n",
    "print(\"Image scraping completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2300166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape smartphone details from Flipkart\n",
    "def scrape_smartphone_details(search_query):\n",
    "    url = f\"https://www.flipkart.com/search?q={search_query.replace(' ', '+')}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        products = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "\n",
    "        smartphone_data = []\n",
    "        for product in products:\n",
    "            try:\n",
    "                brand_name = product.find(\"div\", class_=\"_4rR01T\").text.strip()\n",
    "                smartphone_name = product.find(\"a\", class_=\"IRpwTa\").text.strip()\n",
    "                color = product.find(\"a\", class_=\"IRpwTa\").get(\"title\").split()[-1]\n",
    "                specifications = product.find_all(\"li\", class_=\"rgWa7D\")\n",
    "\n",
    "                ram, storage, primary_camera, secondary_camera, display_size, battery_capacity, price = \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"\n",
    "                for spec in specifications:\n",
    "                    text = spec.text.strip()\n",
    "                    if \"RAM\" in text:\n",
    "                        ram = text\n",
    "                    elif \"ROM\" in text:\n",
    "                        storage = text\n",
    "                    elif \"Primary Camera\" in text:\n",
    "                        primary_camera = text\n",
    "                    elif \"Secondary Camera\" in text:\n",
    "                        secondary_camera = text\n",
    "                    elif \"Display Size\" in text:\n",
    "                        display_size = text\n",
    "                    elif \"Battery Capacity\" in text:\n",
    "                        battery_capacity = text\n",
    "                    elif \"â‚¹\" in text:\n",
    "                        price = text\n",
    "\n",
    "                product_url = product.find(\"a\", class_=\"_1fQZEK\")[\"href\"]\n",
    "                \n",
    "                smartphone_data.append({\n",
    "                    \"Brand Name\": brand_name,\n",
    "                    \"Smartphone Name\": smartphone_name,\n",
    "                    \"Colour\": color,\n",
    "                    \"RAM\": ram,\n",
    "                    \"Storage(ROM)\": storage,\n",
    "                    \"Primary Camera\": primary_camera,\n",
    "                    \"Secondary Camera\": secondary_camera,\n",
    "                    \"Display Size\": display_size,\n",
    "                    \"Battery Capacity\": battery_capacity,\n",
    "                    \"Price\": price,\n",
    "                    \"Product URL\": f\"https://www.flipkart.com{product_url}\",\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping product details: {e}\")\n",
    "\n",
    "        return smartphone_data\n",
    "    else:\n",
    "        print(\"Failed to retrieve the Flipkart search results.\")\n",
    "        return []\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the smartphone you want to search for on Flipkart: \")\n",
    "    smartphone_data = scrape_smartphone_details(search_query)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame(smartphone_data)\n",
    "\n",
    "    # Save the data to a CSV file\n",
    "    df.to_csv(f\"{search_query}_flipkart_smartphones.csv\", index=False)\n",
    "\n",
    "    print(f\"Scraped {len(df)} smartphones and saved to {search_query}_flipkart_smartphones.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc3e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2bfc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import GoogleV3\n",
    "\n",
    "def get_coordinates(city_name):\n",
    "    try:\n",
    "        geolocator = GoogleV3(api_key='YOUR_GOOGLE_API_KEY')\n",
    "        location = geolocator.geocode(city_name)\n",
    "        \n",
    "        if location:\n",
    "            latitude = location.latitude\n",
    "            longitude = location.longitude\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city_name = input(\"Enter the name of the city: \")\n",
    "    \n",
    "    coordinates = get_coordinates(city_name)\n",
    "    \n",
    "    if coordinates:\n",
    "        print(f\"Coordinates for {city_name}: Latitude {coordinates[0]}, Longitude {coordinates[1]}\")\n",
    "    else:\n",
    "        print(f\"Coordinates for {city_name} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape details of gaming laptops from digit.in\n",
    "def scrape_gaming_laptops(url):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the container that holds the laptop details\n",
    "            laptop_container = soup.find('div', class_='right-container')\n",
    "\n",
    "            # Extract laptop details\n",
    "            laptop_name = laptop_container.find('h1', class_='head').text.strip()\n",
    "            laptop_specs = laptop_container.find_all('div', class_='Specs-Wrap')[1].find_all('div', class_='value')\n",
    "            laptop_specs = [spec.text.strip() for spec in laptop_specs]\n",
    "\n",
    "            return {\n",
    "                'Laptop Name': laptop_name,\n",
    "                'Processor': laptop_specs[0],\n",
    "                'OS': laptop_specs[1],\n",
    "                'Display Size': laptop_specs[2],\n",
    "                'Resolution': laptop_specs[3],\n",
    "                'RAM': laptop_specs[4],\n",
    "                'Weight': laptop_specs[5],\n",
    "                'Dimension': laptop_specs[6],\n",
    "                'Graphics Processor': laptop_specs[7],\n",
    "                'Price': laptop_container.find('div', class_='price').text.strip(),\n",
    "                'URL': url\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    \n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all the links to gaming laptop reviews on the page\n",
    "        laptop_links = [a['href'] for a in soup.find_all('a', class_='Best-Pick-Image')]\n",
    "\n",
    "        # List to store laptop details\n",
    "        laptop_details = []\n",
    "\n",
    "        # Iterate through the links and scrape laptop details\n",
    "        for link in laptop_links:\n",
    "            laptop_detail = scrape_gaming_laptops(link)\n",
    "            if laptop_detail:\n",
    "                laptop_details.append(laptop_detail)\n",
    "\n",
    "        # Create a DataFrame from the scraped data\n",
    "        df = pd.DataFrame(laptop_details)\n",
    "\n",
    "        # Save the data to a CSV file\n",
    "        df.to_csv('gaming_laptops.csv', index=False)\n",
    "\n",
    "        print(f\"Scraped {len(laptop_details)} gaming laptops and saved to gaming_laptops.csv\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb96ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape billionaire details from Forbes\n",
    "def scrape_billionaires(url):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the table that holds billionaire details\n",
    "            table = soup.find('table', class_='data')\n",
    "\n",
    "            # Extract billionaire details from the table\n",
    "            billionaires = []\n",
    "            for row in table.find_all('tr')[1:]:\n",
    "                columns = row.find_all('td')\n",
    "                rank = columns[0].text.strip()\n",
    "                name = columns[1].text.strip()\n",
    "                net_worth = columns[2].text.strip()\n",
    "                age = columns[3].text.strip()\n",
    "                citizenship = columns[4].text.strip()\n",
    "                source = columns[5].text.strip()\n",
    "                industry = columns[6].text.strip()\n",
    "\n",
    "                billionaires.append({\n",
    "                    'Rank': rank,\n",
    "                    'Name': name,\n",
    "                    'Net Worth': net_worth,\n",
    "                    'Age': age,\n",
    "                    'Citizenship': citizenship,\n",
    "                    'Source': source,\n",
    "                    'Industry': industry\n",
    "                })\n",
    "\n",
    "            return billionaires\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the link to the full billionaires list\n",
    "        list_link = soup.find('a', class_='d-lg-none').get('href')\n",
    "\n",
    "        # Create the URL for the full list\n",
    "        full_list_url = f\"https://www.forbes.com{list_link}\"\n",
    "\n",
    "        # Scrape billionaire details from the full list URL\n",
    "        billionaire_details = scrape_billionaires(full_list_url)\n",
    "\n",
    "        if billionaire_details:\n",
    "            # Create a DataFrame from the scraped data\n",
    "            df = pd.DataFrame(billionaire_details)\n",
    "\n",
    "            # Save the data to a CSV file\n",
    "            df.to_csv('forbes_billionaires.csv', index=False)\n",
    "\n",
    "            print(f\"Scraped {len(billionaire_details)} billionaires and saved to forbes_billionaires.csv\")\n",
    "        else:\n",
    "            print(\"No data scraped.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35196f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "\n",
    "# Set your API key here\n",
    "API_KEY = \"YOUR_API_KEY\"\n",
    "\n",
    "# Set the video ID for the YouTube video you want to extract comments from\n",
    "VIDEO_ID = \"YOUR_VIDEO_ID\"\n",
    "\n",
    "# Set the maximum number of comments to retrieve\n",
    "MAX_RESULTS = 500\n",
    "\n",
    "# Create a YouTube Data API client\n",
    "youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Function to retrieve video comments\n",
    "def get_video_comments(video_id, max_results=100):\n",
    "    comments = []\n",
    "\n",
    "    # Get video comments in pages of 100 until reaching the desired max_results\n",
    "    page_token = None\n",
    "    while True:\n",
    "        response = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            maxResults=100,\n",
    "            pageToken=page_token\n",
    "        ).execute()\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "            comments.append({\n",
    "                \"Comment\": comment[\"textDisplay\"],\n",
    "                \"Comment Upvotes\": comment.get(\"likeCount\", 0),\n",
    "                \"Time Posted\": comment[\"publishedAt\"]\n",
    "            })\n",
    "\n",
    "            if len(comments) >= max_results:\n",
    "                return comments\n",
    "\n",
    "        if \"nextPageToken\" in response:\n",
    "            page_token = response[\"nextPageToken\"]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    comments = get_video_comments(VIDEO_ID, max_results=MAX_RESULTS)\n",
    "\n",
    "    if comments:\n",
    "        print(f\"Total Comments Extracted: {len(comments)}\")\n",
    "        for idx, comment in enumerate(comments):\n",
    "            print(f\"Comment {idx + 1}:\")\n",
    "            print(f\"Comment: {comment['Comment']}\")\n",
    "            print(f\"Upvotes: {comment['Comment Upvotes']}\")\n",
    "            print(f\"Time Posted: {comment['Time Posted']}\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"No comments extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38127a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape hostel details\n",
    "def scrape_hostels(url):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all the hostel containers\n",
    "            hostel_containers = soup.find_all('div', class_='property-card')\n",
    "\n",
    "            # Extract hostel details\n",
    "            hostels_data = []\n",
    "            for container in hostel_containers:\n",
    "                name = container.find('h2', class_='title').text.strip()\n",
    "                distance = container.find('span', class_='description').text.strip()\n",
    "                ratings = container.find('div', class_='score orange big').text.strip()\n",
    "                total_reviews = container.find('div', class_='reviews').text.strip()\n",
    "                overall_reviews = container.find('div', class_='keyword').text.strip()\n",
    "                privates_price = container.find('a', class_='price privates from').text.strip()\n",
    "                dorms_price = container.find('a', class_='price dorms from').text.strip()\n",
    "                facilities = ', '.join([item.text.strip() for item in container.find_all('li', class_='facility-badge')])\n",
    "                description = container.find('div', class_='card-body').text.strip()\n",
    "\n",
    "                hostels_data.append({\n",
    "                    'Hostel Name': name,\n",
    "                    'Distance from City Centre': distance,\n",
    "                    'Ratings': ratings,\n",
    "                    'Total Reviews': total_reviews,\n",
    "                    'Overall Reviews': overall_reviews,\n",
    "                    'Privates Price': privates_price,\n",
    "                    'Dorms Price': dorms_price,\n",
    "                    'Facilities': facilities,\n",
    "                    'Property Description': description\n",
    "                })\n",
    "\n",
    "            return hostels_data\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.hostelworld.com/hostels/London/England\"\n",
    "\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the total number of pages with hostels\n",
    "        total_pages = int(soup.find('div', class_='pagination__pages').text.strip().split()[-1])\n",
    "\n",
    "        # Create a list to store hostel details\n",
    "        all_hostels_data = []\n",
    "\n",
    "        # Iterate through all pages and scrape hostel details\n",
    "        for page_num in range(1, total_pages + 1):\n",
    "            page_url = f\"{url}?page={page_num}\"\n",
    "            hostels_data = scrape_hostels(page_url)\n",
    "            if hostels_data:\n",
    "                all_hostels_data.extend(hostels_data)\n",
    "\n",
    "        # Create a DataFrame from the scraped data\n",
    "        df = pd.DataFrame(all_hostels_data)\n",
    "\n",
    "        # Save the data to a CSV file\n",
    "        df.to_csv('hostelworld_hostels_london.csv', index=False)\n",
    "\n",
    "        print(f\"Scraped {len(all_hostels_data)} hostels and saved to hostelworld_hostels_london.csv\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
