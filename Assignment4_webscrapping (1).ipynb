{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35570328",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1480098623.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install beautifulsoup4\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0fd5a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a92cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the most viewed videos\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "\n",
    "# Initialize lists to store data\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "# Loop through the rows of the table and extract data\n",
    "for row in table.find_all(\"tr\")[1:]:  # Start from the second row (skipping the header)\n",
    "    columns = row.find_all(\"td\")\n",
    "    rank = columns[0].text.strip()\n",
    "    name = columns[1].text.strip()\n",
    "    artist = columns[2].text.strip()\n",
    "    upload_date = columns[4].text.strip()\n",
    "    views = columns[3].text.strip()\n",
    "    \n",
    "    # Append data to respective lists\n",
    "    rank_list.append(rank)\n",
    "    name_list.append(name)\n",
    "    artist_list.append(artist)\n",
    "    upload_date_list.append(upload_date)\n",
    "    views_list.append(views)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"A) Rank: {rank_list[i]}\")\n",
    "    print(f\"B) Name: {name_list[i]}\")\n",
    "    print(f\"C) Artist: {artist_list[i]}\")\n",
    "    print(f\"D) Upload Date: {upload_date_list[i]}\")\n",
    "    print(f\"E) Views: {views_list[i]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8448d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the BCCI's official website\n",
    "url = \"https://www.bcci.tv/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the section containing the fixtures\n",
    "fixtures_section = soup.find(\"section\", {\"class\": \"international-fixtures\"})\n",
    "\n",
    "# Initialize lists to store data\n",
    "match_title_list = []\n",
    "series_list = []\n",
    "place_list = []\n",
    "date_list = []\n",
    "time_list = []\n",
    "\n",
    "# Loop through the fixtures and extract data\n",
    "fixtures = fixtures_section.find_all(\"div\", {\"class\": \"fixture__info\"})\n",
    "for fixture in fixtures:\n",
    "    match_title = fixture.find(\"span\", {\"class\": \"fixture__format\"}).text.strip()\n",
    "    series = fixture.find(\"span\", {\"class\": \"u-unskewed-text\"}).text.strip()\n",
    "    place = fixture.find(\"p\", {\"class\": \"fixture__additional-info\"}).text.strip()\n",
    "    date = fixture.find(\"span\", {\"class\": \"fixture__date\"}).text.strip()\n",
    "    time = fixture.find(\"span\", {\"class\": \"fixture__time\"}).text.strip()\n",
    "\n",
    "    # Append data to respective lists\n",
    "    match_title_list.append(match_title)\n",
    "    series_list.append(series)\n",
    "    place_list.append(place)\n",
    "    date_list.append(date)\n",
    "    time_list.append(time)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(match_title_list)):\n",
    "    print(f\"A) Match Title: {match_title_list[i]}\")\n",
    "    print(f\"B) Series: {series_list[i]}\")\n",
    "    print(f\"C) Place: {place_list[i]}\")\n",
    "    print(f\"D) Date: {date_list[i]}\")\n",
    "    print(f\"E) Time: {time_list[i]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ea54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the statistics website\n",
    "url = \"http://statisticstimes.com/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the \"Economy of India\" section\n",
    "economy_link = soup.find(\"a\", text=\"Economy of India\")\n",
    "\n",
    "# Extract the URL of the \"Economy of India\" section\n",
    "economy_url = economy_link.get(\"href\")\n",
    "\n",
    "economy_response = requests.get(economy_url)\n",
    "\n",
    "economy_soup = BeautifulSoup(economy_response.content, \"html.parser\")\n",
    "\n",
    "gdp_link = economy_soup.find(\"a\", text=\"GDP of Indian states\")\n",
    "\n",
    "gdp_url = gdp_link.get(\"href\")\n",
    "\n",
    "gdp_response = requests.get(gdp_url)\n",
    "\n",
    "gdp_soup = BeautifulSoup(gdp_response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the State-wise GDP data\n",
    "table = gdp_soup.find(\"table\", {\"id\": \"table_id\"})\n",
    "\n",
    "rank_list = []\n",
    "state_list = []\n",
    "gsdp_18_19_current_prices_list = []\n",
    "gsdp_19_20_current_prices_list = []\n",
    "share_18_19_list = []\n",
    "gdp_billion_list = []\n",
    "\n",
    "\n",
    "for row in table.find_all(\"tr\")[1:]:  # Start from the second row (skipping the header)\n",
    "    columns = row.find_all(\"td\")\n",
    "    rank = columns[0].text.strip()\n",
    "    state = columns[1].text.strip()\n",
    "    gsdp_18_19_current_prices = columns[2].text.strip()\n",
    "    gsdp_19_20_current_prices = columns[3].text.strip()\n",
    "    share_18_19 = columns[4].text.strip()\n",
    "    gdp_billion = columns[5].text.strip()\n",
    "    \n",
    "    # Append data to respective lists\n",
    "    rank_list.append(rank)\n",
    "    state_list.append(state)\n",
    "    gsdp_18_19_current_prices_list.append(gsdp_18_19_current_prices)\n",
    "    gsdp_19_20_current_prices_list.append(gsdp_19_20_current_prices)\n",
    "    share_18_19_list.append(share_18_19)\n",
    "    gdp_billion_list.append(gdp_billion)\n",
    "\n",
    "# Print the data extracted\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"A) Rank: {rank_list[i]}\")\n",
    "    print(f\"B) State: {state_list[i]}\")\n",
    "    print(f\"C) GSDP(18-19) - at current prices: {gsdp_18_19_current_prices_list[i]}\")\n",
    "    print(f\"D) GSDP(19-20) - at current prices: {gsdp_19_20_current_prices_list[i]}\")\n",
    "    print(f\"E) Share(18-19): {share_18_19_list[i]}\")\n",
    "    print(f\"F) GDP($ billion): {gdp_billion_list[i]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the GitHub homepage\n",
    "url = \"https://github.com/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the list of trending repositories\n",
    "trending_repositories = soup.find_all(\"article\", class_=\"Box-row\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "repository_title_list = []\n",
    "repository_description_list = []\n",
    "contributors_count_list = []\n",
    "language_used_list = []\n",
    "\n",
    "# Loop through the trending repositories and extract data\n",
    "for repo in trending_repositories:\n",
    "    # Repository title\n",
    "    repo_title = repo.find(\"h1\", class_=\"h3\").text.strip()\n",
    "    \n",
    "    # Repository description\n",
    "    repo_description_tag = repo.find(\"p\", class_=\"col-9\")\n",
    "    repo_description = repo_description_tag.text.strip() if repo_description_tag else \"No description available\"\n",
    "    \n",
    "    #Contributors count (if available)\n",
    "    repo_contributors_tag = repo.find(\"a\", class_=\"Link--muted\")\n",
    "    repo_contributors = repo_contributors_tag.text.strip() if repo_contributors_tag else \"0\"\n",
    "    \n",
    "    # Language used (if available)\n",
    "    repo_language_tag = repo.find(\"span\", itemprop=\"programmingLanguage\")\n",
    "    repo_language = repo_language_tag.text.strip() if repo_language_tag else \"Not specified\"\n",
    "    \n",
    "    # Append data to respective lists\n",
    "    repository_title_list.append(repo_title)\n",
    "    repository_description_list.append(repo_description)\n",
    "    contributors_count_list.append(repo_contributors)\n",
    "    language_used_list.append(repo_language)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(repository_title_list)):\n",
    "    print(f\"A) Repository Title: {repository_title_list[i]}\")\n",
    "    print(f\"B) Repository Description: {repository_description_list[i]}\")\n",
    "    print(f\"C) Contributors Count: {contributors_count_list[i]}\")\n",
    "    print(f\"D) Language Used: {language_used_list[i]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb2347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize a Selenium webdriver (make sure you have the appropriate driver installed)\n",
    "driver = webdriver.Chrome(executable_path='/path/to/chromedriver')\n",
    "\n",
    "# URL of the Billboard homepage\n",
    "url = \"https://www.billboard.com/\"\n",
    "\n",
    "# Open the Billboard homepage\n",
    "driver.get(url)\n",
    "\n",
    "# Click on the \"Charts\" option\n",
    "charts_option = driver.find_element(By.XPATH, '//span[text()=\"Charts\"]')\n",
    "charts_option.click()\n",
    "\n",
    "# Click on the \"Hot 100\" link\n",
    "hot_100_link = driver.find_element(By.XPATH, '//a[text()=\"Hot 100\"]')\n",
    "hot_100_link.click()\n",
    "\n",
    "# Wait for the page to load (you might need to adjust the wait time)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Get the HTML content of the Hot 100 page\n",
    "hot_100_page_html = driver.page_source\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(hot_100_page_html, \"html.parser\")\n",
    "\n",
    "# Find the list of top 100 songs\n",
    "songs_list = soup.find_all(\"li\", class_=\"chart-list__element\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "song_name_list = []\n",
    "artist_name_list = []\n",
    "last_week_rank_list = []\n",
    "peak_rank_list = []\n",
    "weeks_on_board_list = []\n",
    "\n",
    "# Loop through the top 100 songs and extract data\n",
    "for song in songs_list:\n",
    "    song_name = song.find(\"span\", class_=\"chart-element__information__song text--truncate color--primary\").text.strip()\n",
    "    artist_name = song.find(\"span\", class_=\"chart-element__information__artist text--truncate color--secondary\").text.strip()\n",
    "    last_week_rank = song.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--last\").text.strip()\n",
    "    peak_rank = song.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--peak\").text.strip()\n",
    "    weeks_on_board = song.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--week\").text.strip()\n",
    "    \n",
    "    # Append data to respective lists\n",
    "    song_name_list.append(song_name)\n",
    "    artist_name_list.append(artist_name)\n",
    "    last_week_rank_list.append(last_week_rank)\n",
    "    peak_rank_list.append(peak_rank)\n",
    "    weeks_on_board_list.append(weeks_on_board)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(song_name_list)):\n",
    "    print(f\"A) Song Name: {song_name_list[i]}\")\n",
    "    print(f\"B) Artist Name: {artist_name_list[i]}\")\n",
    "    print(f\"C) Last Week Rank: {last_week_rank_list[i]}\")\n",
    "    print(f\"D) Peak Rank: {peak_rank_list[i]}\")\n",
    "    print(f\"E) Weeks on Board: {weeks_on_board_list[i]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0408edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Sample URL (replace with the actual URL)\n",
    "url = \"https://example.com/highest-selling-novels\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find and extract the information\n",
    "book_names = soup.find_all(\"div\", class_=\"book-name\")\n",
    "author_names = soup.find_all(\"div\", class_=\"author-name\")\n",
    "volumes_sold = soup.find_all(\"div\", class_=\"volumes-sold\")\n",
    "publishers = soup.find_all(\"div\", class_=\"publisher\")\n",
    "genres = soup.find_all(\"div\", class_=\"genre\")\n",
    "\n",
    "# Store the scraped data in lists or data structures\n",
    "book_name_list = [name.text.strip() for name in book_names]\n",
    "author_name_list = [author.text.strip() for author in author_names]\n",
    "volumes_sold_list = [volume.text.strip() for volume in volumes_sold]\n",
    "publisher_list = [publisher.text.strip() for publisher in publishers]\n",
    "genre_list = [genre.text.strip() for genre in genres]\n",
    "\n",
    "# Print or process the scraped data as needed\n",
    "for i in range(len(book_name_list)):\n",
    "    print(f\"A) Book Name: {book_name_list[i]}\")\n",
    "    print(f\"B) Author Name: {author_name_list[i]}\")\n",
    "    print(f\"C) Volumes Sold: {volumes_sold_list[i]}\")\n",
    "    print(f\"D) Publisher: {publisher_list[i]}\")\n",
    "    print(f\"E) Genre: {genre_list[i]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd776530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the IMDb page\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the list of TV series\n",
    "series_list = soup.find_all(\"div\", class_=\"lister-item-content\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "name_list = []\n",
    "year_span_list = []\n",
    "genre_list = []\n",
    "run_time_list = []\n",
    "ratings_list = []\n",
    "votes_list = []\n",
    "\n",
    "# Loop through the TV series and extract data\n",
    "for series in series_list:\n",
    "    name = series.find(\"h3\", class_=\"lister-item-header\").a.text.strip()\n",
    "    \n",
    "    year_span = series.find(\"span\", class_=\"lister-item-year\").text.strip()\n",
    "    \n",
    "    genre = series.find(\"span\", class_=\"genre\").text.strip()\n",
    "    \n",
    "    run_time = series.find(\"span\", class_=\"runtime\").text.strip()\n",
    "    \n",
    "    ratings = series.find(\"strong\").text.strip()\n",
    "    \n",
    "    votes = series.find(\"span\", attrs={\"name\": \"nv\"})[\"data-value\"].strip()\n",
    "    \n",
    "    # Append data to respective lists\n",
    "    name_list.append(name)\n",
    "    year_span_list.append(year_span)\n",
    "    genre_list.append(genre)\n",
    "    run_time_list.append(run_time)\n",
    "    ratings_list.append(ratings)\n",
    "    votes_list.append(votes)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(name_list)):\n",
    "    print(f\"A) Name: {name_list[i]}\")\n",
    "    print(f\"B) Year Span: {year_span_list[i]}\")\n",
    "    print(f\"C) Genre: {genre_list[i]}\")\n",
    "    print(f\"D) Run Time: {run_time_list[i]}\")\n",
    "    print(f\"E) Ratings: {ratings_list[i]}\")\n",
    "    print(f\"F) Votes: {votes_list[i]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the UCI Machine Learning Repository homepage\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the \"View ALL Data Sets\" page\n",
    "datasets_link = soup.find(\"a\", href=\"ml/datasets.php\")\n",
    "\n",
    "# Extract the URL of the \"View ALL Data Sets\" page\n",
    "datasets_url = url + datasets_link[\"href\"]\n",
    "\n",
    "# Send an HTTP GET request to the \"View ALL Data Sets\" page\n",
    "datasets_response = requests.get(datasets_url)\n",
    "\n",
    "# Parse the HTML content of the \"View ALL Data Sets\" page\n",
    "datasets_soup = BeautifulSoup(datasets_response.content, \"html.parser\")\n",
    "\n",
    "# Find the list of datasets\n",
    "datasets_list = datasets_soup.find_all(\"tr\", valign=\"top\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "dataset_name_list = []\n",
    "data_type_list = []\n",
    "task_list = []\n",
    "attribute_type_list = []\n",
    "no_of_instances_list = []\n",
    "no_of_attributes_list = []\n",
    "year_list = []\n",
    "\n",
    "# Loop through the datasets and extract data\n",
    "for dataset in datasets_list:\n",
    "    columns = dataset.find_all(\"td\")\n",
    "    dataset_name = columns[0].text.strip()\n",
    "    data_type = columns[1].text.strip()\n",
    "    task = columns[2].text.strip()\n",
    "    attribute_type = columns[3].text.strip()\n",
    "    no_of_instances = columns[4].text.strip()\n",
    "    no_of_attributes = columns[5].text.strip()\n",
    "    year = columns[6].text.strip()\n",
    "    \n",
    "    # Append data to respective lists\n",
    "    dataset_name_list.append(dataset_name)\n",
    "    data_type_list.append(data_type)\n",
    "    task_list.append(task)\n",
    "    attribute_type_list.append(attribute_type)\n",
    "    no_of_instances_list.append(no_of_instances)\n",
    "    no_of_attributes_list.append(no_of_attributes)\n",
    "    year_list.append(year)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(dataset_name_list)):\n",
    "    print(f\"A) Dataset Name: {dataset_name_list[i]}\")\n",
    "    print(f\"B) Data Type: {data_type_list[i]}\")\n",
    "    print(f\"C) Task: {task_list[i]}\")\n",
    "    print(f\"D) Attribute Type: {attribute_type_list[i]}\")\n",
    "    print(f\"E) No of Instances: {no_of_instances_list[i]}\")\n",
    "    print(f\"F) No of Attributes: {no_of_attributes_list[i]}\")\n",
    "    print(f\"G) Year: {year_list[i]}\")\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
